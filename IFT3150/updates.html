<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="keywords" content="HTML, CSS">
    <meta name="description" content="Project for the IFT 3150 course at the
	 University of Montreal">
    <title>IFT 3150 - Project in Computer Science</title>
    <link rel="stylesheet" href="style_ift3150.css">
</head>

<body style="padding: 30px 10vw; background-color:skyblue;">

<h1 class="title">
    Updates
</h1>

<!-- ************************************************* -->
<!-- Workload 6 - Doodad II -->
<!-- ************************************************* -->
<h2>Workload 6 - Setup for Experiments</h2>

<p class="text">
    Week of July 25th and August 1st<br>
    <b>Deadline</b>: August 7th
</p>

<p class="text">
    - Create a virtual machine Linux (with anaconda)<br>
    - Connect to remote computer (ssh blue)<br>
    - Connect to GPU's remote computer for experiments<br>
</p>

<hr class="style">

<!-- ************************************************* -->
<!-- Workload 5 - Doodad -->
<!-- ************************************************* -->
<h2>Workload 5 - Doodad</h2>

<p class="text">
    Week of July 17th <br>
    <b>Deadline</b>: July 25th
</p>

<p class="text">
    During the past workloads, the rlkit library was modified locally in my computer, but it became
    quite hard to keep track all the changes. I decided to fork the original repositories for rlkit
    and doodad in order to track down the changes I've done and the issues that I've encountered. From now on,
    all the changes that were made in the Github repositories will be shown here too. For the links of the repo,
    consult <i>Notes and References</i>.
</p>

<p class="text">
    <b><u>Note:</u></b> The changes on the Github repositories will be shown at the very end of each workload.
</p>

<p class="text">
    I will also use doodad library in order to run experiments on different computers. This week's purpose is to also
    understand the code from doodad and test Atari experiments locally using doodad. The goal of workloads 5 and 6 is to complete
    the setup for doodad in order to run experiments from the lab computer.
</p>

<img src = "../images/w5-github.png" onclick="this.src=full_size.gif"><br>

<hr class="style">

<!-- ************************************************* -->
<!-- Workload 4 - Docker -->
<!-- ************************************************* -->
<h2>Workload 4 - Docker</h2>

<p class="text">
    Week of July 11th <br>
    <b>Deadline</b>: July 17th
</p>

<p class="text">
    This week's purpose was to focus on learning Docker and Dockerfile in order to use them eventually
    in the main project. Finally, I created an image for the rlkit library that was inspired by SMiRL-Code library's
    Dockerfile. Besides learning Docker, I also took some notes related to Meta-RL
    (talk given Chelsea Finn) and learned some more Pytorch basics.<br>
</p>

<p class="text">
    <b><u>Update on the topic of the project:</u></b><br>
    Also, we were initially planning on applying the concept of generalization in different humanoids
    for control tasks (morphology-agnostic learning). However, due to time restrictions, I decided
    to fully focus on the concept of generalization using different Atari environments since I
    already wrote two different CNN policies for different Atari environments.<br>
    <br>
    The goal here is to experiment on three different environments using CNN policy. The agent trains
    on three different games and it needs to learn the fourth game by itself (a new game that the agent
    has never seen before) based on its experience.
</p>

<hr class="style">


<!-- ************************************************* -->
<!-- Workload 3 - Atari -->
<!-- ************************************************* -->
<h2>Workload 3 - Atari</h2>

<p class="text">
    Week of June 27th and July 4th <br>
    <b>Deadline</b>: July 8th
</p>

<p class="text">
    During the previous workload, I was testing the rlkit code with DQN-Cartpole (Project I), but no changes
    were made in the actual code. The professor and I have decided to work on another small
    project (exercise), by modifying some parts of the code: (1) Change the Cartpole environment into
    an Atari one (Breakout), (2) Preprocess the images from the breakout game, (3) Create a new policy
    from scratch using Pytorch CNN.
</p>

<p class="text">
    In order to prepare for the implementation in Pytorch, I followed a playlist of Pytorch tutorials
    (until workload 5). This also helps me to revise the basics of deep learning and backpropagation.
    In this workload, I suggested two different CNN Policies for the training of Breakout Game that are shown
    in the figure (at the end of this block). The plots for the training aren't done since this workload
    was mainly focused on building a CNN Policy. For the Atari Breakout mini-project, our next goal is
    to test the two models with a large number of epochs with CPU and GPU. I tested in my CPU for a very small
    amount of epochs, so the performance was obviously not great. In order to evaluate our CNN policy,
    we'll have to wait until we finish setup the doodad library and the docker image in order to run
    experiments from the lab computer.
</p>

<p class="text">
    We also had a brief introduction to Dockerfile this week, a topic that will be covered in the next workload.
</p>

<img src = "../images/cnn_atari_models.png" alt = "CNN Models for Atari Breakout">
<i><br>Taken from Notes and References/Project II - DQN Atari</i>


<hr class="style">

<!-- ************************************************* -->
<!-- Workload 2 - DQN -->
<!-- ************************************************* -->
<h2>Workload 2 - DQN</h2>

<p class="text">
    Weeks of June 13th and June 20th <br>
    <b>Deadline</b>: June 26th
</p>

<p class="text">
    This section of workload focuses on gathering information that is related to the project, which
    is expanding the concept of generalization in RL. I've found two papers that mainly talk about DQN
    algorithm that is applied in multiple situations: the first paper talks about fluid mechanics,
    whereas the second one talks about the Atari Environment. Furthermore, we have found a paper
    that mainly talks about generalization in RL with zero-policy transfer. In terms of readings,
    the goal for the upcoming weeks is to find papers that talks more about multi-task and hierarchical
    learning.
</p>

<p class="text">
    I also start testing the RLKIT package by testing the DQN Algorithm with the cartpole environment.
    The RLKIT package mainly uses PyTorch to implement different methods, so I've decided
    to apply the same principles but with another library, which is stable-baseline3.
</p>

<img src = "../images/updates_cartpole.png" alt = "Graphs for PPO Model in Cartpole Environment">
<i><br>Taken from Notes and References/Project I - DQN Cartpole</i>

<p class="text">
    - Based on the value loss plot, we can see that the agent starts to learn between 0-25k timesteps and then the function decreases once the reward is stabilized.
    <br>- Based on the policy gradient loss plot, we can see a sudden drop in 35k timesteps, which means the training is successful during this specific amount of timesteps. We observe that the loss increases significantly once it reaches over 35k timesteps.
    <br>- Based on the explained variance, at 35k timesteps, the value is 0.9633, which is the highest value of explained variance in the plot. However, it is still not superior to 1, which means our PPO model can be improved with other alternatives.
</p>

<hr class="style">


<!-- ************************************************* -->
<!-- Workload 1 - Basics of RL -->
<!-- ************************************************* -->
<h2>Workload 1 - Basics of RL</h2>

<p class="text">
    Weeks of May 30th and June 6th <br>
    <b>Deadline</b>: June 12th
</p>

<p class="text">I start by taking notes about the basics of reinforcement learning. The lectures that I
    listened are given by professor Steven Brunton. The lectures give an overview of numerous RL methods that
    are model-based and model-free. I've decided to focus on the algorithm of Q-learning and DQN, since it
    will be useful for the main project. Since the mathematics and the logic behind the algorithms are quite
    complex, I've decided to link other papers or online tutorials that will explain deeply some of the concepts
    that I find interesting (i.e. Q-Learning). </p>

<p class="text">In terms of implementation, I start by using the <mark>stable-baseline3</mark> package and OpenAI. I will
    complete three small projects before diving into the concept of generalization in RL: Atari Game,
    Autonomous Driving and Creating a custom environment with Gym libraries.</p>

<hr class="style">


<!-- ************************************************* -->
<!-- DISCUSSION AND PREPARATION -->
<!-- ************************************************* -->
<h2>Discussion and Preparation</h2>

<p class="text">
    Week of May 23rd <br>
</p>

<p class="text">We review the basics and the importance of deep learning and reinforcement
    learning with the professor (back propagation, overfitting, etc.). We then discuss about the
    upcoming tasks that need to accomplish in order to be familiar with reinforcement learning.
    I decide to start with simple projects that are related with reinforcement learning.</p>

<p class="text">Once I have completed some small projects, we then decide to work on the main project, which
    is morphology traning across different environments (expanding generalization of RL). If we have more
    time in the future, we can try to train with a fixed training data (off-policy idea). </p>
<hr class="style">


<!-- ************************************************* -->
<!-- TITLE AND ABSTRACT -->
<!-- ************************************************* -->
<h2>Title and Abstract</h2>

<p class="text">
    Week of May 16th <br>
    <b>Deadline</b>: May 20th
</p>

<p class="text"> We start by discussing the topic of the project. We have found one paper
    that talks about the concept of improving reinforcement learning with morphology-agnostic
    learning. The tasks that we've decided to train on were not confirmed yet, but we're going
    to use this paper as inspiration for this project. I've found some ideas that I can use as
    different tasks to train on different agents, like playing chess, the Atari Game, or
    training humanoids for control tasks (application of morphology-agnostic learning), but these
    are all brainstormed ideas for now.</p>

<p style="text-align:right"><a href="#top">Top of the page</a></p>
<hr class="style">

</body>
</html>